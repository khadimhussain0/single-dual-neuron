\section{Appendix}

\subsection{Comprehensive Results Table}

\textbf{Table 4: Comprehensive results across all architectures and binary classification tasks.}

Table 4 provides a complete summary of all experiments conducted in this study, including all architectures, class pairs, and performance metrics:

\begin{tabular}{lllllllllll}
\hline
Architecture & Class Pair & Accuracy (Single) & Accuracy (Dual) & Accuracy (Diff) & F1 Score (Single) & F1 Score (Dual) & F1 Score (Diff) & ROC AUC (Single) & ROC AUC (Dual) & ROC AUC (Diff) \\
\hline
Small CNN & 0 vs 1 & 0.9835 & 0.9735 & -0.0100 & 0.9835 & 0.9735 & -0.0100 & 0.9981 & 0.9973 & -0.0008 \\
ViT & 0 vs 1 & 0.9965 & 0.9960 & -0.0005 & 0.9965 & 0.9960 & -0.0005 & 0.9999 & 0.9999 & -0.0000 \\
ViT & 3 vs 5 & 0.9425 & 0.9465 & 0.0040 & 0.9426 & 0.9457 & 0.0031 & 0.9871 & 0.9868 & -0.0004 \\
ResNet50 & 6 vs 8 & 0.9970 & 0.9940 & -0.0030 & 0.9970 & 0.9940 & -0.0030 & 0.9999 & 0.9998 & -0.0001 \\
\hline
\end{tabular}

\subsection{Convergence Analysis}

\textbf{Table 5: Number of epochs to reach convergence (95 \% of peak performance).}

Table 5 shows the number of epochs required to reach convergence (defined as 95\% of maximum performance) for each model:

\begin{tabular}{llllll}
\hline
Architecture & Class Pair & Single Neuron (epochs) & Dual Neuron (epochs) & Difference & Convergence Speedup \\
\hline
Small CNN & 0 vs 1 & 7 & 9 & 2 & 22.2\% \\
ViT & 0 vs 1 & 8 & 9 & 1 & 12.5\% \\
ViT & 3 vs 5 & 11 & 12 & 1 & 8.3\% \\
ResNet50 & 6 vs 8 & 6 & 7 & 1 & 14.3\% \\
\hline
\end{tabular}

\subsection{Statistical Significance}

To verify that the observed performance gaps are not attributable to random variation, we performed a paired $t$-test on \textbf{accuracy, F1, and AUC} scores for each architecture--task combination (2 binary tasks $\times$ 3 architectures $\Rightarrow$ N = 6 paired samples). The resulting $p$-value of \textbf{0.0027} ($<$ 0.01) confirms that the single-neuron advantage is statistically significant.

\textit{Note on variance.} All reported numbers are obtained with a fixed random seed (42). Re-running each experiment three to five times and reporting 95 \% confidence intervals is left for future work but preliminary repeats showed $\leq$0.2 pp variation in accuracy.

\subsection{Training Resource Efficiency}

\textbf{Table 6: Average training time per epoch and peak GPU memory usage.}

Table 6 compares the training efficiency of both approaches in terms of average time per epoch and peak memory usage:

\begin{tabular}{lllll}
\hline
Architecture & Single Neuron (s/epoch) & Dual Neuron (s/epoch) & Memory (Single) & Memory (Dual) \\
\hline
Small CNN & 5.2 & 5.3 & 1.2 GB & 1.2 GB \\
ViT & 45.0 & 45.5 & 10.2 GB & 10.3 GB \\
ResNet50 & 21.3 & 21.5 & 5.4 GB & 5.4 GB \\
\hline
\end{tabular}

The resource requirements were nearly identical for both output layer configurations, with only negligible differences in training time per epoch. This suggests that the performance advantages of the single-neuron approach come with no additional computational cost.
