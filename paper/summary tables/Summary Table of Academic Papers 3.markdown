| S. No | Author(s) & Year | Title of Paper | Method/Model Used | Dataset Name | Results/Findings |
| --- | --- | --- | --- | --- | --- |
| 13 | S. Yang, C. Zhang, Y. Bao, J. Yang, and W. Wu, 2020 | Binary Output Layer of Extreme Learning Machine for Solving Multi-Class Classification Problems | Extreme Learning Machine (ELM) with a new "binary approach" for output layer design | NA | The binary approach performs as well as the one-to-one approach but uses fewer output nodes and hidden-output weights |
| 14 | M. Klimo, P. Lukáč, and P. Tarábek, 2021 | Deep Neural Networks Classification via Binary Error-Detecting Output Codes | A novel method using Zadeh fuzzy logic to train binary output codes holistically, using linear block codes (e.g., Cyclic Redundancy Check—CRC) for error detection | CIFAR-10, MNIST, Fashion MNIST, CIFAR-100, white noise | Achieves similar accuracy as one-hot encoding with softmax, with CRC7 and Zadeh decision achieving 92.44% for CNN1, 93.97% for CNN2, 91.56% for ResNet20v2. Reliability improves with error detection, and out-of-distribution rejection rates are reported. |
| 15 | S. Maharjan, A. Alsadoon, P. W. C. Prasad, and A. K. Singh, 2020 | A Novel Enhanced Softmax Loss Function for Brain Tumour Detection Using Deep Learning | Convolutional Neural Network (CNN) with modified softmax loss function and regularization | A dataset of 3064 images from 233 patients, including 708 meningioma, 1426 glioma, and 930 pituitary tumors | Classification accuracy improved by \~2% compared to other solutions, and processing time reduced by 40\~50 ms compared to current solutions |
| 16 | R. Hu, B. Tian, S. Yin, and S. Wei, 2018 | Efficient Hardware Architecture of Softmax Layer in Deep Neural Network | NA (paper content not accessible) | NA (paper content not accessible) | NA (paper content not accessible) |
| 17 | S. Khan, M. Naseer, H. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, 2022 | Transformers in Vision: A Survey | NA (paper content not accessible) | NA (paper content not accessible) | NA (paper content not accessible) |
| 18 | Z. Liu, Y. Lin, Y. Cao, et al., 2021 | Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows | Swin Transformer | ImageNet-1K, COCO, ADE20K | ImageNet-1K: 87.3 top-1 accuracy; COCO: +2.7 box AP, +2.6 mask AP; ADE20K: +3.2 mIoU |
| 19 | X. Chu, Z. Tian, B. Zhang, et al., 2021 | Twins: Revisiting the Design of Spatial Attention in Vision Transformers | Twins-PCPVT and Twins-SVT | NA (not explicitly mentioned) | Achieves excellent performance on image-level classification, dense detection, and segmentation |
| 20 | A. Vaswani, N. Shazeer, N. Parmar, et al., 2017 | Attention Is All You Need | Transformer, a novel network architecture based solely on attention mechanisms | NA (not explicitly mentioned) | Achieves 27.5 BLEU on English-to-German translation, improves over existing best ensemble result by over 1 BLEU; on English-to-French translation, outperforms previous single state-of-the-art model by 0.7 BLEU, achieving 41.1 BLEU |
