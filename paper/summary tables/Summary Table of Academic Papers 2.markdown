# Summary Table of Deep Learning Papers

| S. No | Author(s) & Year | Title of Paper | Method/Model Used | Dataset Name | Results/Findings |
|-------|------------------|----------------|-------------------|--------------|------------------|
| 7 | Steffen Eger, Paul Youssef, Iryna Gurevych, 2019 | Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks | Comparison of 21 activation functions across 8 NLP tasks using MLPs, CNNs, and RNNs | MR, SUBJ, TREC, AM, NG, R8, POS, TL-AM | Penalized tanh performs most stably across tasks; swish also performs well but is less stable; replacing sigmoid/tanh in LSTMs with penalized tanh improves performance by 2pp on a challenging NLP task. |
| 8 | Ameya D. Jagtap, George Em Karniadakis, 2022 | How important are activation functions in regression and classification? A survey, performance comparison, and future directions | Survey and comparison of activation functions in neural networks, including PIML with PINNs | MNIST, CIFAR-10, CIFAR-100, and various regression datasets for PIML | ReLU and variants are state-of-the-art for classification but underperform in PIML; Sine, Tanh, Swish, and adaptive functions perform better in PIML; JAX outperforms TensorFlow and PyTorch in PIML tasks. |
| 9 | Behnam Asadi, Hui Jiang, 2020 | On Approximation Capabilities of ReLU Activation and Softmax Output Layer in Neural Networks | Theoretical analysis of approximation capabilities of neural networks with ReLU and softmax | N/A (theoretical work) | Proved universal approximation theorems for ReLU activation and softmax output layers in neural networks. |
| 10 | Shiv Ram Dubey, Satish Kumar Singh, Bidyut Baran Chaudhuri, 2021 | Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark | Survey and benchmark of 18 activation functions across different neural networks and data types | CIFAR10, CIFAR100, IWSLT 2016, WMT 2014, LibriSpeech | Different activation functions perform best for different tasks; PAU converges fastest, while PDELU and SRS increase training time. |
| 11 | Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou, 2021 | Training data-efficient image transformers & distillation through attention | Data-efficient image Transformers (DeiT) with token-based distillation | ImageNet, CIFAR-10, CIFAR-100, Oxford-102 Flowers, Stanford Cars, iNaturalist-2018, iNaturalist-2019 | DeiT-B achieves 83.1% top-1 accuracy on ImageNet without external data, up to 85.2% with distillation; strong transfer learning performance on various datasets. |
| 12 | Mahmoud Khalil, Ahmad Khalil, Alioune Ngom, 2023 | A Comprehensive Study of Vision Transformers in Image Classification Tasks | Study of various Vision Transformers (ViT, Swin, DeiT, CaiT, MViT, iGPT, Model Soups) | ImageNet, ImageNet-21k, ImageNet-ReaL, VOC07, Oxford Flowers, CIFAR-10, CIFAR-100, Oxford-IIIT Pets, YFCC100M, MS COCO, Open Images, Places365, iNat18, JFT-300M | ViT models achieve high accuracy (e.g., ViT-H/14: 88.55% on ImageNet); discusses challenges like overfitting, class imbalance, and robustness to adversarial examples. |