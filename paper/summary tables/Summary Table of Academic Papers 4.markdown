| S. No | Author(s) & Year | Title of Paper | Method/Model Used | Dataset Name | Results/Findings |
| --- | --- | --- | --- | --- | --- |
| 21 | Y. Wu, Y. Xiao, H. Tang, et al., 2020 | Visual Transformers: Token-based Image Representation and Processing for Computer Vision | Visual Transformer (VT), VT-ResNets, VT-FPN | ImageNet, COCO-stuff, Look-Into-Person (LIP) | VT-ResNets improve top-1 accuracy by 4.6 to 7 points over ResNet baselines, reduce FLOPs by up to 6.9x; VT-FPN achieves higher mIoU with reduced FLOPs. |
| 22 | J. Maur√≠cio, A. de Carvalho, J. R. S. Tavares, and P. Martins, 2023 | Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review | Literature review comparing Vision Transformers (ViT) and Convolutional Neural Networks (CNN) | Various datasets (ImageNet, CIFAR-10, CIFAR-100, etc.) | ViT outperforms CNN in noise/augmented images due to self-attention; ViT better with small datasets; CNN generalizes better with smaller datasets; ViT has less overhead and shorter training time. |
| 23 | Y. Huo, K. Jin, J. Cai, H. Xiong, and J. Pang, 2023 | Vision Transformer (ViT)-Based Applications in Image Classification | NA (behind paywall) | NA | NA |
| 24 | Z. Qi, Y. Zhang, R. Li, et al., 2022 | Privacy-Preserving Image Classification Using Vision Transformer | Vision Transformer with block-wise encryption | CIFAR-10, CIFAR-100 | Achieved 96.64% accuracy on CIFAR-10 and 84.42% on CIFAR-100; robust against attacks with high security. |
| 25 | H. Zheng, Z. Yang, W. Liu, J. Liang, and Y. Li, 2015 | Improving Deep Neural Networks Using Softplus Units | NA (behind paywall) | NA | NA |
| 26 | D. Pedamonti, 2018 | Comparison of Non-Linear Activation Functions for Deep Neural Networks on MNIST Classification Task | Deep neural networks with ReLU, Leaky ReLU, ELU, SELU | MNIST | ELU performed better than other activation functions in most cases, with validation accuracies up to 0.983. |
| 27 | G. Alcantara, 2017 | Empirical Analysis of Non-Linear Activation Functions for Deep Neural Networks in Classification Tasks | Deep Neural Networks with Sigmoid, ReLU, Leaky ReLU, ELU, SELU | MNIST | ELU with learning rate 0.02 achieved highest accuracy (0.980) with Glorot initialization. |
| 28 | S. Sharma, S. Sharma, and A. Athaiya, 2017 | Activation Functions in Neural Networks | NA (behind paywall) | NA | NA |
